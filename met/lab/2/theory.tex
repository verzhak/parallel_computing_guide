
\mysubsubsection{Введение}

В предыдущей лабораторной работе были рассмотрены принципы организации параллельных и распределенных вычислений в многопроцессорных системах с разделяемой памятью. Вычислительные потоки в данных системах взаимодействуют друг с другом с помощью областей оперативной памяти, одинаково доступных каждому из потоков. Таковой способ взаимодействия вычислительных потоков чреват возникновением большого числа явных или скрытых ошибок, связанных с одновременным доступом к одной и той же ячейке памяти несколькими потоками. Кроме того, таковой способ взаимодействия вычислительных потоков очевидно не может быть реализован в тех ситуациях, когда необходимо реализовать параллельные или распределенные вычисления с помощью вычислительных кластеров - с помощью многопроцессорных систем с распределенной памятью.

Для реализации параллельных и распределенных вычислений с помощью многопроцессорных систем с распределенной памятью используют модель вычислительных потоков, взаимодействующих друг с другом путем пересылки сообщений. Таковой способ взаимодействия вычислительных потоков позволяет программисту избавится от ошибок, связанных с параллельным доступом вычислительных потоков к разделяемой памяти, и объединить некоторые задачи - например, ожидание вычислительным потоком поступления данных от другого вычислительного потока и чтение этих данных.

\mysubsubsection{Многопоточное программирование в \gl с взаимодействием вычислительных потоков путем обмена сообщениями}

Как и в случае взаимодействия вычислительных потоков через разделяемую память, самый низкоуровневый (и доступный во всех установка \gl) способ организации многопоточных вычислений с взаимодействием вычислительных потоков путем обмена сообщениями в \gl основан на использовании системных вызовов.

Помимо системных вызовов fork, wait, waitpid, wait4, использующихся для создания процессов системы, реализующих вычислительные потоки, и ожидания завершения процессов системы, в таковых многопоточных приложения необходимо использовать системные вызовы socket, bind, accept, send, recv, sendto, recvfrom, close - словом, системные вызовы, позволяющие организовать взаимодействие (удаленных) процессов с помощью сокетов. Для взаимодействия процессов могут использоваться как сокеты типа AF\_UNIX, позволяющие организовать взаимодействие процессов, запущенных в одной и той же системе, так и типов AF\_INET, AF\_INET6 и прочие, позволяющие организовать взаимодействие процессов, запущенных в различных, удаленных друг от друга, вычислительных системах (сокеты типа AF\_INET используют протоколы транспортного уровня стека протоколов TCP/IPv4, сокеты AF\_INET6 - протоколы транспортного уровня стека протоколов TCP/IPv6). В некоторых вычислительных системах (к которым относятся современные суперкомпьютеры) взаимодействие процессов может основываться на более быстрой технологии, чем протоколы TCP, SCTP и UDP. В вычислительных кластерах, состоящих из нескольких персональных компьютеров, объединенных в вычислительную сеть, построенную, например, с помощью технологии Ethernet, использование протоколов TCP, SCTP и UDP представляется наиболее целесообразным - и именно сетевое взаимодействие становится наиболее затратным по времени в таковых вычислительных кластерах (как следствие, возникает необходимость минимизировать количество пересылок сообщений между вычислительными потоками и минимизировать объемы пересылаемых сообщений).

С точки зрения программиста, организация параллельных и распределенных вычислений с взаимодействием вычислительных потоков путем пересылки сообщений с помощью системных вызовов \gl является значительно более трудоемкой задачей по сравнению с организацией параллельных и распределенных вычислений с взаимодействием вычислительных потоков, взаимодействующих через разделяемую память, по следующим причинам:

\begin{itemize}

	\item Программист должен самостоятельно реализовать собственный протокол обмена сообщениями поверх протоколов транспортного уровня стека протоколов TCP/IP;
	\item Программист должен самостоятельно отслеживать все ошибки, могущие возникнуть в процессе сетевого обмена;
	\item Программист должен самостоятельно реализовать пересылку исполняемого кода на удаленные системы;
	\item Программист должен самостоятельно реализовать функционал запуска исполняемого кода на удаленных системах.

\end{itemize}

К счастью, на настоящий момент у программиста имеется несколько способов существенно упростить процесс организации параллельных и распределенных вычислений:

\begin{itemize}

	\item Использование специальных библиотек (например, библиотек, реализующих стандарт MPI - библиотеки MPICH, OpenMPI и другие);
	\item Использование языков параллельного программирования (например, язык программирования Erlang).

\end{itemize}

В настоящей лабораторной работе рассматривается способ организации параллельных и распределенных вычислений с взаимодействием вычислительных потоков путем обмена сообщениями с помощью библиотеки OpenMPI, реализующей стандарт MPI.

\mysubsubsection{Стандарт MPI. Библиотека OpenMPI}

Стандарт MPI (Message Passing Interface, интерфейс обмена сообщениями) был разработан группой ученых во главе с профессором Иллинойского университета Уильямом Гроуппом в 1992. В настоящее время стандарт MPI дорабатывается некоммерческим объединением <<MPI Forum>> \cite{mpi-forum}. Существуют версии стандарта MPI для языков программирования C, C++, Fortran.

В настоящее время имеется несколько реализаций стандарта MPI:

\begin{itemize}

	\item Библиотека MPICH;
	\item Библиотека OpenMPI \cite{openmpi};
	\item Библиотека Intel MPI;
	\item Библиотека HP MPI;
	\item Прочие библиотеки.

\end{itemize}

В настоящей лабораторной работе используется реализация стандарта MPI для языка программирования C из состава библиотеки OpenMPI. В качестве компилятора для языка программирования C в настоящей лабораторной работе используется компилятор GNU C Compiler из состава коллекции компиляторов GNU Compiler Collection.

Стандарт MPI позволяет программисту реализовать параллельные вычисления в несколько потоков. Вычислительные потоки объединяются в группы потоков (для простоты будем рассматривать группу потоков, идентифицируемую константой компилятора \linebreak MPI\_COMM\_WORLD - вопросы разделения потоков на группы, объединение групп потоков и прочие вопросы, связанные с группировкой потоков, выходят за рамки лабораторной работы). Потоки нумеруются, считая от 0 (нулевой поток - главный поток).

Каждый вычислительный поток получает для выполнения один и тот же бинарный код, разграничение выполняемого потоками кода может быть реализовано с помощью условных операторов и проверки номера потока.

В начале выполняемого потоками кода должен находится вызов функции {\bf MPI\_Init()}, прототип которой приведен в листинге \ref{listing:mpi-init}. Функция MPI\_Init() инициализирует параллельную часть вычислительного потока.

\mylistingbegin{mpi-init}{Функция MPI\_Init()}
\begin{lstlisting}

int MPI_Init(int * argc, char *** argv);

\end{lstlisting}
\mylistingend

Функция MPI\_Init() обладает следующими параметрами:

\begin{itemize}

	\item argc - указатель на переменную, хранящую количество аргументов, переданных вычислительному потоку при его запуске;
	\item argv - указатель на массив строк, хранящий аргументы, переданные вычислительному потоку при его запуске.

\end{itemize}

Значения параметров argc и argv функции MPI\_Init() должны быть равны адресам параметров argc и argv функции main() вычислительного потока.

Функция MPI\_Init() возвращает значение константы компилятора MPI\_SUCCESS в случае своего успешного завершения и значение, отличное от значения указанной константы компилятора, в случае неудачного своего завершения. То же возвращают и все прочие функции стандарта MPI, рассмотренные в настоящей лабораторной работе, если только не указано обратное.

По завершению параллельной части своего кода вычислительный поток должен вызвать функцию {\bf MPI\_Finalize()}, прототип которой приведен в листинге \ref{listing:mpi-finalize}.

\mylistingbegin{mpi-finalize}{Функция MPI\_Finalize()}
\begin{lstlisting}

int MPI_Finalize();

\end{lstlisting}
\mylistingend

Вычислительный поток может получить свой номер, вызвав функцию \linebreak {\bf MPI\_Comm\_rank()}, прототип которой приведен в листинге \ref{listing:mpi-comm-rank}. Для получения же числа потоков в определенной группе потоков вычислительный поток может воспользоваться функцией {\bf MPI\_Comm\_size()}, прототип которой приведен в листинге \ref{listing:mpi-comm-size}.

\mylistingbegin{mpi-comm-rank}{Функция MPI\_Comm\_rank()}
\begin{lstlisting}

int MPI_Comm_rank(MPI_COMM_WORLD, int * rank);

\end{lstlisting}
\mylistingend

\mylistingbegin{mpi-comm-size}{Функция MPI\_Comm\_size()}
\begin{lstlisting}

int MPI_Comm_size(MPI_COMM_WORLD, int * size);

\end{lstlisting}
\mylistingend

Функция MPI\_Comm\_rank() присвоит номер потока переменной, на которую указывает ее параметр rank. Функция MPI\_Comm\_size() присвоит количество потоков в группе MPI\_COMM\_WORLD переменной, на которую указывает ее параметр size.

Для отправки и приема сообщений вычислительные потоки могут использовать следующий функционал стандарта MPI:

\begin{itemize}

	\item Функции {\bf MPI\_Send()} и {\bf MPI\_Recv()}.

	Функция MPI\_Send() позволяет вычислительному потоку отправить сообщение целевому вычислительному потоку или всем вычислительным потокам, входящим в целевую группу потоков. Прототип функции MPI\_Send() приведен в листинге \ref{listing:mpi-send}.

\mylistingbegin{mpi-send}{Функция MPI\_Send()}
\begin{lstlisting}

int MPI_Send(void * buf, int count, MPI_Datatype type, int dst, int tag, MPI_COMM_WORLD);

\end{lstlisting}
\mylistingend

	Функция MPI\_Send() отправляет count элементов типа type из буфера buf. В качестве значения параметра type могут использоваться значения следующих констант компилятора:

	\begin{itemize}

		\item MPI\_CHAR - char;
		\item MPI\_SHORT - short int;
		\item MPI\_INT - int;
		\item MPI\_LONG - long int;
		\item MPI\_UNSIGNED\_CHAR - unsigned char;
		\item MPI\_UNSIGNED\_SHORT - unsigned short int;
		\item MPI\_UNSIGNED - unsigned int;
		\item MPI\_UNSIGNED\_LONG - unsigned long int;
		\item MPI\_FLOAT - float;
		\item MPI\_DOUBLE - double;
		\item MPI\_LONG\_DOUBLE - long double.

	\end{itemize}

	Таким образом, функция MPI\_Send() отправит целевому вычислительному потоку (потокам) первые (count * sizeof(type)) байт из буфера buf потока - отправителя.

	Значение параметра dst функции MPI\_Send() суть есть номер того вычислительного потока, которому отправляется сообщение. Для широковещательной рассылки сообщений в качестве значения параметра dst необходимо указать значение константы компилятора MPI\_ANY\_SOURCE.

	Значение параметра tag указывает тег сообщения. Тег сообщение - положительное целое число, идентифицирующее данное сообщение (тип данного сообщения).

	Поток - супервизор помещает сообщение, отправленное некоторому вычислительному потоку, в очередь сообщений данного потока, из которого оно может быть выбрано потоком с помощью функции MPI\_Recv(). При этом соблюдается временная очередность выборки сообщений из очереди - первым будет выбрано то сообщение от указанного источника с указанным тегом, которое было помещено в очередь раньше остальных. Прототип функции MPI\_Recv() приведен в листинге \ref{listing:mpi-recv}.

\mylistingbegin{mpi-recv}{Функция MPI\_Recv()}
\begin{lstlisting}

int MPI_Recv(void * buf, int count, MPI_Datatype type, int src, int tag, MPI_COMM_WORLD, MPI_Status * status);

\end{lstlisting}
\mylistingend

	Функция MPI\_Recv() принимает сообщение с тегом tag от вычислительного потока с номером src, помещая count элементов типа type сообщения в буфер buf. В переменную, на которую указывает параметр status, помещается информация о параметрах принятого сообщения.

	В случае, если сообщения с указанным тегом от указанного источника в очереди сообщений отсутствует, вычислительный поток блокируется в ожидании такового сообщения.

	Для приема сообщений с любым тегом в качестве значения параметра tag необходимо указать значение константы компилятора MPI\_ANY\_TAG.

	Для приема сообщений от любого источника в качестве значения параметра src необходимо указать значение константы компилятора MPI\_ANY\_SOURCE;

	\item Функции {\bf MPI\_Bcast()} и {\bf MPI\_Gather()}.

	Функция MPI\_Bcast() позволяет организовать широковещательную рассылку сообщения. Прототип функции MPI\_Bcast() приведен в листинге \ref{listing:mpi-bcast}.

\mylistingbegin{mpi-bcast}{Функция MPI\_Bcast()}
\begin{lstlisting}

int MPI_Bcast(void * buf, int count, MPI_Datatype type, int src, MPI_COMM_WORLD);

\end{lstlisting}
\mylistingend

	Функция MPI\_Bcast() отправляет count элементов типа type из буфера buf все процессам из группы процессов MPI\_COMM\_WORLD.

	Номер потока, отправляющего сообщение, указывается параметром src. Данный поток также получит отправляемое им сообщение.

	Функция MPI\_Gather() позволяет организовать сбор данных со всех вычислительных потоков одним из них. Прототип функции MPI\_Gather() приведен в листинге \ref{listing:mpi-gather}.

\mylistingbegin{mpi-gather}{Функция MPI\_Gather()}
\begin{lstlisting}

int MPI_Gather(void * src_buf, int src_count, MPI_Datatype src_type, void * dst_buf, int dst_count, MPI_Datatype dst_type, int dst, MPI_COMM_WORLD);

\end{lstlisting}
\mylistingend

	Вычислительный поток с номером dst принимает сообщения от всех вычислительных потоков группы MPI\_COMM\_WORLD (в том числе, и от себя самого).

	Каждый вычислительный поток отправляет src\_count элементов типа src\_type из буфера src\_buf.

	Вычислительный поток с номером dst принимает по dst\_count элементов типа dst\_type от каждого вычислительного потока и помещает принятые данные в буфере dst\_buf, располагая блоки данных в порядке возрастания номеров отправивших их вычислительных потоков.

\end{itemize}

При разработке многопоточных приложений, использующих стандарт MPI, полезным будет также использование функции {\bf MPI\_Barrier()}, позволяющей синхронизировать выполнение вычислительных потоков. Прототип функции MPI\_Barrier() приведен в листинге \ref{listing:mpi-barrier}.

\mylistingbegin{mpi-barrier}{Функция MPI\_Barrier()}
\begin{lstlisting}

int MPI_Barrier(MPI_COMM_WORLD);

\end{lstlisting}
\mylistingend

Вычислительный поток, входящий в группу потоков MPI\_COMM\_WORLD, блокируется функцией MPI\_Barrier() до тех пор, пока все остальные потоки данной группы не вызовут функцию MPI\_Barrier();

\mysubsubsection{Сборка и запуск программ, использующих библиотеку OpenMPI}

В исходный код программы, использующей библиотеку OpenMPI для организации параллельных или распределенных вычислений, необходимо включить заголовочный файл mpi.h.

Для сборки программы, использующей библиотеку OpenMPI для организации параллельных или распределенных вычислений, с помощью компилятора GNU C Compiler, необходимо использовать оболочку к означенному компилятору, предоставляемую библиотекой OpenMPI. Данная оболочка реализована в виде исполняемого файла mpicc. Исполняемый файл mpicc может быть запущен с аргументами, передаваемыми исполняемому файлу gcc компилятора GNU C Compiler - аргументы, необходимые для корректного связывания программы с библиотекой OpenMPI, исполняемый файл mpicc добавит самостоятельно в процессе своего выполнения.

Процесс запуска программы, использующей библиотеку OpenMPI, на вычислительном кластере состоит из следующих шагов:

\begin{enumerate}

	\item Настройка среды запуска\footnote{В лаборатории настройка среды запуска выполняется лаборантами.}.

	Вычислительные потоки, запускаемые на отдельных вычислительных системах, входящих в состав кластера, обмениваются между собой сообщениями по тем или иным протоколам.

	Существуют протоколы обмена сообщениями, специально разработанные (и, следовательно, оптимизированные) для организации вычислительных кластеров. В случае же организации кластера на основе нескольких персональных компьютеров, объединенных друг с другом в вычислительную сеть, в качестве протоколов обмена сообщениями используются следующие протоколы:

	\begin{itemize}

		\item Протокол RSH;
		
		\item Протокол SSH.

			Протокол SSH дополнительно применяет шифрование для защиты пересылаемых данных, что негативно сказывается на временных характеристиках процесса обмена сообщениями между вычислительными потоками.

	\end{itemize}

	Следует помнить, что программа будет запущена на удаленных вычислительных системах от имени того же пользователя, от имени которого она запускается в головной вычислительной системе кластера. Таким образом, данный пользователь на удаленных вычислительных системах должен существовать и ему должно быть разрешено удаленное подключение без ввода пароля (в случае протокола SSH - аутентификация по ключу);

	\item Сборка программы;

	\item Рассылка исполняемого файла программы на удаленные вычислительные системы.

	Исполняемый файл программы может быть передан удаленным вычислительным системам следующими способами:

	\begin{itemize}

		\item Передачей на съемном носителе;
		\item Передачей с помощью утилиты ncat (nc; при этом требуется удаленный доступ на целевую вычислительную систему по протоколам RSH или SSH);

	\end{itemize}

	\item Запуск программы.

	Для запуска программы используется утилита mpirun, входящая в состав библиотеки OpenMPI. Утилита mpirun принимает следующие аргументы:

	\begin{itemize}

		\item -mca orte\_rsh\_agent rsh - использование для обмена сообщениями протокола RSH;
		\item -mca orte\_rsh\_agent ssh - использование для обмена сообщениями протокола SSH;
		\item -mca orte\_rsh\_agent "ssh : rsh" - использование для обмена сообщениями протокола SSH или, если подключение по SSH завершилось неудачей, протокола RSH;
		\item -n NUM - предписание запустить NUM вычислительных потоков;
		\item -H IP\_1,IP\_2,...,IP\_N - объединить в вычислительный кластер удаленные вычислительные системы с перечисленными IP-адресами;
		\item -loadbalance - равномерное распределение вычислительных потоков по удаленным вычислительным системам (на одной вычислительной системе может быть запущено несколько вычислительных потоков);
		\item -x PATH=\verb|"|/bin:/usr/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin:DIR\verb|"| - предписание запустить программу с указанным значением переменной окружения PATH.

		Здесь DIR - абсолютный путь к каталогу, содержащему исполняемый файл программы, на удаленной вычислительной системе. Если исполняемый файл программы находится в разных каталогах на разных вычислительных системах, то необходимо продолжить перечисление через двоеточие.

		На удаленных вычислительных системах исполняемый файл программы будет запущен с помощью утилиты orted, поэтому принципиально важно нахождение ее исполняемого файла в каталогах, перечисленных в переменной окружения PATH;

		\item Имя исполняемого файла программы.

		Исполняемый файл программы должен находится в одном из каталогов, перечисленных в переменной PATH.

	\end{itemize}

	Стандартные потоки вывода и ошибок каждого вычислительного потока будут переправлены в тот терминал, в котором программа была запущена на выполнение на кластере с помощью утилиты mpirun.

\end{enumerate}

