
\mysubsubsection{Введение}

В предыдущей лабораторной работе были рассмотрены принципы организации параллельных и распределенных вычислений в многопроцессорных системах с разделяемой памятью. Вычислительные потоки в данном случае взаимодействуют друг с другом с помощью областей оперативной памяти, одинаково доступных каждому из потоков. Таковой способ взаимодействия вычислительных потоков чреват возникновением большого числа явных или скрытых ошибок, связанных с одновременным доступом к одной и той же ячейке памяти несколькими потоками. Кроме того, таковой способ взаимодействия вычислительных потоков очевидно не может быть реализован в тех случаях, когда необходимо организовать параллельные или распределенные вычисления на вычислительном кластере - то есть в многопроцессорной системе с распределенной памятью.

Для организации параллельных и распределенных вычислений в многопроцессорных системах с распределенной памятью используют модель вычислительных потоков, взаимодействующих друг с другом путем обмена сообщениями. Таковой способ взаимодействия вычислительных потоков позволяет программисту избавится от ошибок, связанных с параллельным доступом вычислительных потоков к разделяемой памяти, и, в некоторых случаях, существенно сократить объем кода программы - например, объединить код ожидания вычислительным потоком поступления данных от другого вычислительного потока и код чтения этих данных.

\mysubsubsection{Многопоточное программирование в \gl с взаимодействием вычислительных потоков путем обмена сообщениями}

Как и в случае взаимодействия вычислительных потоков через разделяемую память, самый низкоуровневый (и доступный во всех установках \gl) способ организации многопоточных вычислений с взаимодействием вычислительных потоков путем обмена сообщениями в \gl основан на использовании системных вызовов.

Помимо системных вызовов fork, wait, waitpid, wait4, использующихся для создания процессов системы, реализующих вычислительные потоки, и ожидания завершения процессов системы, в таковых многопоточных приложениях необходимо использовать системные вызовы socket, bind, accept, send, recv, sendto, recvfrom, close - словом, системные вызовы, позволяющие организовать взаимодействие (удаленных) процессов с помощью сокетов. Для взаимодействия процессов могут использоваться как сокеты типа AF\_UNIX, позволяющие организовать взаимодействие процессов, запущенных в одной и той же вычислительной системе, так и типов AF\_INET, AF\_INET6 и прочих, позволяющих организовать взаимодействие процессов, запущенных в различных, удаленных друг от друга, вычислительных системах (сокеты типа AF\_INET используют протоколы транспортного уровня стека протоколов TCP/IPv4, сокеты AF\_INET6 - протоколы транспортного уровня стека протоколов TCP/IPv6). В некоторых вычислительных системах (к которым относятся современные суперкомпьютеры) взаимодействие процессов может основываться на более быстрых технологиях, чем протоколы TCP, SCTP и UDP. В вычислительных кластерах, состоящих из нескольких персональных компьютеров, объединенных в вычислительную сеть, построенную, например, с помощью технологии Ethernet, использование протоколов TCP, SCTP, UDP и протоколов, основанных на перечисленных протоколах, представляется единственно возможным - и именно сетевое взаимодействие становится наиболее затратным по времени в таковых вычислительных кластерах (как следствие, возникает необходимость минимизировать количество пересылок сообщений между вычислительными потоками и минимизировать объемы пересылаемых сообщений).

С точки зрения программиста, организация параллельных и распределенных вычислений с взаимодействием вычислительных потоков путем пересылки сообщений с помощью системных вызовов \gl является задачей, значительно более трудоемкой по сравнению с организацией с помощью системных вызовов \gl параллельных и распределенных вычислений с взаимодействием вычислительных потоков через разделяемую память, по следующим причинам:

\begin{itemize}

	\item Программист должен самостоятельно реализовать собственный протокол обмена сообщениями поверх протоколов транспортного уровня стека протоколов TCP/IP;
	\item Программист должен самостоятельно отслеживать все ошибки, могущие возникнуть в процессе сетевого обмена;
	\item Программист должен самостоятельно реализовать пересылку исполняемого кода на удаленные системы;
	\item Программист должен самостоятельно реализовать функционал запуска исполняемого кода в удаленных системах.

\end{itemize}

У программиста имеется несколько способов существенно упростить процесс организации параллельных и распределенных вычислений. К таковым способам относятся:

\begin{itemize}

	\item Использование специальных библиотек (например, библиотек, реализующих стандарт MPI - библиотеки MPICH, OpenMPI и другие);
	\item Использование языков параллельного программирования (например, язык программирования Erlang).

\end{itemize}

В настоящей лабораторной работе рассматривается способ организации параллельных и распределенных вычислений с взаимодействием вычислительных потоков путем обмена сообщениями с помощью библиотеки OpenMPI, реализующей стандарт MPI.

\mysubsubsection{Стандарт MPI. Библиотека OpenMPI}

Стандарт MPI (Message Passing Interface, интерфейс обмена сообщениями) был разработан группой ученых во главе с профессором Иллинойского университета Уильямом Гроуппом в 1992-м году. В настоящее время стандарт MPI дорабатывается некоммерческим объединением <<MPI Forum>> \cite{mpi-forum}. Существуют версии стандарта MPI для языков программирования C, C++, Fortran.

В настоящее время имеется несколько реализаций стандарта MPI:

\begin{itemize}

	\item Библиотека MPICH;
	\item Библиотека OpenMPI \cite{openmpi};
	\item Библиотека Intel MPI;
	\item Библиотека HP MPI;
	\item Прочие библиотеки.

\end{itemize}

В настоящей лабораторной работе рассматривается реализация стандарта MPI библиотекой OpenMPI для языка программирования C. В качестве компилятора для языка программирования C в настоящей лабораторной работе используется компилятор GNU C Compiler из состава коллекции компиляторов GNU Compiler Collection.

Стандарт MPI позволяет программисту реализовать параллельные вычисления в несколько потоков. Вычислительные потоки объединяются в группы потоков (для простоты будем рассматривать группу потоков, идентифицируемую константой компилятора \linebreak MPI\_COMM\_WORLD - вопросы разделения потоков на группы, объединение групп потоков и прочие вопросы, связанные с группировкой потоков, выходят за рамки данной лабораторной работы). Потоки нумеруются от 0 (нулевой поток - главный поток).

Каждый вычислительный поток выполняет один и тот же бинарный код. Разграничение выполняемого потоками кода может быть реализовано с помощью условных операторов и проверки номера потока.

В начале выполняемого потоками кода должен находится вызов функции {\bf MPI\_Init()}, прототип которой приведен в листинге \ref{listing:mpi-init}. Функция MPI\_Init() инициализирует параллельную часть вычислительного потока.

\mylistingbegin{mpi-init}{Функция MPI\_Init()}
\begin{lstlisting}

int MPI_Init(int * argc, char *** argv);

\end{lstlisting}
\mylistingend

Функция MPI\_Init() обладает следующими параметрами:

\begin{itemize}

	\item argc - указатель на переменную, хранящую количество аргументов, переданных вычислительному потоку при его запуске;
	\item argv - указатель на массив строк, хранящий аргументы, переданные вычислительному потоку при его запуске.

\end{itemize}

Значения параметров argc и argv функции MPI\_Init() должны быть равны адресам параметров argc и argv функции main() вычислительного потока.

Функция MPI\_Init() возвращает значение константы компилятора MPI\_SUCCESS в случае своего успешного завершения и значение, отличное от значения указанной константы компилятора, в случае неудачного своего завершения. То же возвращают и все прочие функции стандарта MPI, рассмотренные в настоящей лабораторной работе, если только не указано обратное.

По завершении параллельной части кода вычислительный поток должен вызвать функцию {\bf MPI\_Finalize()}, прототип которой приведен в листинге \ref{listing:mpi-finalize}.

\mylistingbegin{mpi-finalize}{Функция MPI\_Finalize()}
\begin{lstlisting}

int MPI_Finalize();

\end{lstlisting}
\mylistingend

Вычислительный поток может получить свой номер, вызвав функцию \linebreak {\bf MPI\_Comm\_rank()}, прототип которой приведен в листинге \ref{listing:mpi-comm-rank}. Для получения числа потоков в определенной группе потоков вычислительный поток может воспользоваться функцией {\bf MPI\_Comm\_size()}, прототип которой приведен в листинге \ref{listing:mpi-comm-size}.

\mylistingbegin{mpi-comm-rank}{Функция MPI\_Comm\_rank()}
\begin{lstlisting}

int MPI_Comm_rank(MPI_COMM_WORLD, int * rank);

\end{lstlisting}
\mylistingend

\mylistingbegin{mpi-comm-size}{Функция MPI\_Comm\_size()}
\begin{lstlisting}

int MPI_Comm_size(MPI_COMM_WORLD, int * size);

\end{lstlisting}
\mylistingend

Функция MPI\_Comm\_rank() присвоит номер потока переменной, на которую указывает параметр rank. Функция MPI\_Comm\_size() присвоит значение количества потоков в группе MPI\_COMM\_WORLD переменной, на которую указывает параметр size.

Для отправки и приема сообщений вычислительные потоки могут использовать следующий функционал стандарта MPI:

\begin{itemize}

	\item Функции {\bf MPI\_Send()} и {\bf MPI\_Recv()}.

	Функция MPI\_Send() позволяет вычислительному потоку отправить сообщение целевому вычислительному потоку или всем вычислительным потокам, входящим в целевую группу потоков. Прототип функции MPI\_Send() приведен в листинге \ref{listing:mpi-send}.

\mylistingbegin{mpi-send}{Функция MPI\_Send()}
\begin{lstlisting}

int MPI_Send(void * buf, int count, MPI_Datatype type, int dst, int tag, MPI_COMM_WORLD);

\end{lstlisting}
\mylistingend

	Функция MPI\_Send() отправляет count элементов типа type из буфера buf. В качестве значения параметра type могут использоваться значения следующих констант компилятора:

	\begin{itemize}

		\item MPI\_CHAR - char;
		\item MPI\_SHORT - short int;
		\item MPI\_INT - int;
		\item MPI\_LONG - long int;
		\item MPI\_UNSIGNED\_CHAR - unsigned char;
		\item MPI\_UNSIGNED\_SHORT - unsigned short int;
		\item MPI\_UNSIGNED - unsigned int;
		\item MPI\_UNSIGNED\_LONG - unsigned long int;
		\item MPI\_FLOAT - float;
		\item MPI\_DOUBLE - double;
		\item MPI\_LONG\_DOUBLE - long double.

	\end{itemize}

	Таким образом, функция MPI\_Send() отправит целевому вычислительному потоку (потокам) первые (count * sizeof(ctype)) байт из буфера buf потока - отправителя, где ctype - тип языка программирования C, соответствующий значению type.

	Значение параметра dst функции MPI\_Send() суть есть номер того вычислительного потока, которому отправляется сообщение. Для широковещательной рассылки сообщений в качестве значения параметра dst необходимо указать значение константы компилятора MPI\_ANY\_SOURCE.

	Значение параметра tag указывает тег сообщения. Тег сообщение - положительное целое число, идентифицирующее данное сообщение (тип данного сообщения).

	Поток - супервизор помещает сообщение, отправленное некоторому вычислительному потоку, в очередь сообщений данного потока, из которого оно может быть выбрано потоком с помощью функции MPI\_Recv(). При этом соблюдается временная очередность выборки сообщений из очереди - первым будет выбрано то сообщение от указанного источника с указанным тегом, которое было помещено в очередь раньше остальных. Прототип функции MPI\_Recv() приведен в листинге \ref{listing:mpi-recv}.

\mylistingbegin{mpi-recv}{Функция MPI\_Recv()}
\begin{lstlisting}

int MPI_Recv(void * buf, int count, MPI_Datatype type, int src, int tag, MPI_COMM_WORLD, MPI_Status * status);

\end{lstlisting}
\mylistingend

	Функция MPI\_Recv() принимает сообщение с тегом tag от вычислительного потока с номером src, помещая count элементов типа type сообщения в буфер buf. В переменную, на которую указывает параметр status, помещается информация о параметрах принятого сообщения.

	В случае, если сообщение с указанным тегом от указанного источника в очереди сообщений отсутствует, вычислительный поток блокируется в ожидании такового сообщения.

	Для приема сообщений с любым тегом в качестве значения параметра tag необходимо указать значение константы компилятора MPI\_ANY\_TAG.

	Для приема сообщений от любого источника в качестве значения параметра src необходимо указать значение константы компилятора MPI\_ANY\_SOURCE;

	\item Функции {\bf MPI\_Bcast()} и {\bf MPI\_Gather()}.

	Функция MPI\_Bcast() позволяет организовать широковещательную рассылку сообщения. Прототип функции MPI\_Bcast() приведен в листинге \ref{listing:mpi-bcast}.

\mylistingbegin{mpi-bcast}{Функция MPI\_Bcast()}
\begin{lstlisting}

int MPI_Bcast(void * buf, int count, MPI_Datatype type, int src, MPI_COMM_WORLD);

\end{lstlisting}
\mylistingend

	Функция MPI\_Bcast() отправляет count элементов типа type из буфера buf все вычислительным потокам из группы потоков MPI\_COMM\_WORLD.

	Номер потока, отправляющего сообщение, указывается параметром src. Данный поток также получит отправляемое им сообщение.

	Функция MPI\_Gather() позволяет организовать сбор данных от всех вычислительных потоков одним из них. Прототип функции MPI\_Gather() приведен в листинге \ref{listing:mpi-gather}.

\mylistingbegin{mpi-gather}{Функция MPI\_Gather()}
\begin{lstlisting}

int MPI_Gather(void * src_buf, int src_count, MPI_Datatype src_type, void * dst_buf, int dst_count, MPI_Datatype dst_type, int dst, MPI_COMM_WORLD);

\end{lstlisting}
\mylistingend

	Вычислительный поток с номером dst принимает сообщения от всех вычислительных потоков группы MPI\_COMM\_WORLD (в том числе, и от себя самого).

	Каждый вычислительный поток отправляет src\_count элементов типа src\_type из буфера src\_buf.

	Вычислительный поток с номером dst принимает по dst\_count элементов типа dst\_type от каждого вычислительного потока и помещает принятые данные в буфере dst\_buf, располагая блоки данных в порядке возрастания номеров отправивших их вычислительных потоков.

\end{itemize}

При разработке многопоточных приложений, использующих стандарт MPI, полезным будет также использование функции {\bf MPI\_Barrier()}, позволяющей синхронизировать выполнение вычислительных потоков. Прототип функции MPI\_Barrier() приведен в листинге \ref{listing:mpi-barrier}.

\mylistingbegin{mpi-barrier}{Функция MPI\_Barrier()}
\begin{lstlisting}

int MPI_Barrier(MPI_COMM_WORLD);

\end{lstlisting}
\mylistingend

Вычислительный поток, входящий в группу потоков MPI\_COMM\_WORLD, блокируется функцией MPI\_Barrier() до тех пор, пока все остальные потоки данной группы не вызовут функцию MPI\_Barrier();

\mysubsubsection{Сборка и запуск программ, использующих библиотеку OpenMPI}

В исходный код программы, использующей библиотеку OpenMPI, необходимо включить заголовочный файл mpi.h.

Для сборки программы, использующей библиотеку OpenMPI, с помощью компилятора GNU C Compiler необходимо использовать оболочку к означенному компилятору, предоставляемую библиотекой OpenMPI. Данная оболочка присутствует в системе в виде исполняемого файла mpicc. Исполняемый файл mpicc запустит компилятор GNU C Compiler, добавить к аргументам, указанным программистом, аргументы, необходимые для корректного связывания программы с библиотекой OpenMPI.

Процесс запуска программы, использующей библиотеку OpenMPI, в вычислительном кластере состоит из следующих шагов:

\begin{enumerate}

	\item Настройка среды запуска\footnote{В лаборатории настройка среды запуска выполняется лаборантами.}.

	Вычислительные потоки, запускаемые на отдельных вычислительных системах, входящих в состав кластера, обмениваются между собой сообщениями по тем или иным протоколам.

	Существуют протоколы обмена сообщениями, специально разработанные (и, следовательно, оптимизированные) для организации вычислительных кластеров. В случае же организации кластера путем объединения нескольких персональных компьютеров, соединенных друг с другом в вычислительную сеть, в качестве протоколов обмена сообщениями библиотекой OpenMPI используются следующие протоколы:

	\begin{itemize}

		\item Протокол RSH;
		
		\item Протокол SSH.

			Протокол SSH использует шифрование для защиты пересылаемых данных, что негативно сказывается на временных характеристиках процесса обмена сообщениями между вычислительными потоками.

	\end{itemize}

	Следует помнить, что программа будет запущена на удаленных вычислительных системах от имени того же пользователя, от имени которого она запускается в головной вычислительной системе кластера. Таким образом, данный пользователь на удаленных вычислительных системах должен существовать и ему должен быть разрешен удаленный доступ без ввода пароля (в случае протокола SSH - аутентификация по ключу);

	\item Сборка программы;

	\item Рассылка исполняемого файла программы на удаленные вычислительные системы.

	Исполняемый файл программы может быть передан на удаленные вычислительные системы следующими способами:

	\begin{itemize}

		\item Передачей на съемном носителе;
		\item Передачей с помощью утилиты ncat (nc; при этом требуется удаленный доступ к целевой вычислительной системе по протоколам RSH или SSH);

	\end{itemize}

	\item Запуск программы.

	Для запуска программы используется утилита mpirun, входящая в состав библиотеки OpenMPI. Утилита mpirun принимает следующие аргументы:

	\begin{itemize}

		\item -mca orte\_rsh\_agent rsh - использование для обмена сообщениями протокола RSH;
		\item -mca orte\_rsh\_agent ssh - использование для обмена сообщениями протокола SSH;
		\item -mca orte\_rsh\_agent \verb|"|ssh : rsh\verb|"| - использование для обмена сообщениями протокола SSH или, если подключение по SSH завершилось неудачей, протокола RSH;
		\item -n NUM - предписание запустить NUM вычислительных потоков;
		\item -H IP\_1,IP\_2,...,IP\_N - объединить в вычислительный кластер удаленные вычислительные системы с перечисленными IP-адресами;
		\item -loadbalance - равномерное распределение вычислительных потоков по удаленным вычислительным системам (на одной вычислительной системе может быть запущено несколько вычислительных потоков);
		\item -x PATH=\verb|"|/bin:/usr/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin:DIR\verb|"| - предписание запускать вычислительные потоки с указанным значением переменной окружения PATH.

		Здесь DIR - абсолютный путь к каталогу, содержащему исполняемый файл программы. Если исполняемый файл программы в разных вычислительных системах находится в разных каталогах, то перечисление каталогов необходимо продолжить через двоеточие.

		В удаленных вычислительных системах исполняемый файл программы будет запущен с помощью утилиты orted, поэтому принципиально важно нахождение ее исполняемого файла в каталогах, перечисленных в переменной окружения PATH;

		\item Имя исполняемого файла программы.

		Исполняемый файл программы должен находится в одном из каталогов, перечисленных в переменной PATH.

	\end{itemize}

	Стандартные потоки вывода и ошибок каждого вычислительного потока будут переправлены в тот терминал, в котором программа была запущена на выполнение с помощью утилиты mpirun.

\end{enumerate}

