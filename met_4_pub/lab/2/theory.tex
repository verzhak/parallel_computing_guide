
\mysubsubsection{Введение}

Для организации параллельных и распределенных вычислений в многопроцессорных системах с распределенной памятью используют модель вычислительных потоков, взаимодействующих друг с другом путем обмена сообщениями. Таковой способ взаимодействия вычислительных потоков позволяет программисту избавится от ошибок, связанных с параллельным доступом вычислительных потоков к разделяемой памяти, и, в некоторых случаях, существенно сократить объем кода программы - например, объединить код ожидания вычислительным потоком поступления данных от другого вычислительного потока и код чтения этих данных.

У программиста имеется несколько способов существенно упростить процесс организации параллельных и распределенных вычислений. К таковым способам относятся:

\begin{itemize}
	
	\item использование специальных библиотек (например, библиотек, реализующих стандарт MPI - библиотеки MPICH, OpenMPI и другие);
	\item использование языков параллельного программирования (например, язык программирования Erlang).

\end{itemize}

\mysubsubsection{Стандарт MPI. Библиотека OpenMPI}

Стандарт MPI (Message Passing Interface, интерфейс обмена сообщениями) был разработан группой ученых во главе с профессором Иллинойского университета Уильямом Гроуппом в 1992-м году. В настоящее время стандарт MPI дорабатывается некоммерческим объединением <<MPI Forum>> \cite{mpi-forum}. Существуют версии стандарта MPI для языков программирования C, C++, Fortran.

В настоящее время имеется несколько реализаций стандарта MPI - в настоящей лабораторной работе рассматривается реализация стандарта MPI библиотекой OpenMPI для языка программирования C. В качестве компилятора для языка программирования C в настоящей лабораторной работе используется компилятор GNU C Compiler из состава коллекции компиляторов GNU Compiler Collection.

Стандарт MPI позволяет программисту реализовать параллельные вычисления в несколько потоков. Вычислительные потоки объединяются в группы потоков (для простоты будем рассматривать группу потоков, идентифицируемую константой компилятора \linebreak MPI\_COMM\_WORLD - вопросы разделения потоков на группы, объединение групп потоков и прочие вопросы, связанные с группировкой потоков, выходят за рамки данной лабораторной работы). Потоки нумеруются от 0 (нулевой поток - главный поток).

Каждый вычислительный поток выполняет один и тот же бинарный код. Разграничение выполняемого потоками кода может быть реализовано с помощью условных операторов и проверки номера потока.

В начале выполняемого потоками кода должен находится вызов функции MPI\_Init() (листинг \ref{listing:mpi-init}). Функция MPI\_Init() инициализирует параллельную часть вычислительного потока.

\mylistingbegin{mpi-init}{Функция MPI\_Init()}
\begin{lstlisting}

int MPI_Init(int * argc, char *** argv);

\end{lstlisting}
\mylistingend

Функция MPI\_Init() обладает следующими параметрами:

\begin{itemize}

	\item argc - указатель на переменную, хранящую количество аргументов, переданных вычислительному потоку при его запуске;
	\item argv - указатель на массив строк, хранящий аргументы, переданные вычислительному потоку при его запуске.

\end{itemize}

Значения параметров argc и argv функции MPI\_Init() должны быть равны адресам в памяти параметров argc и argv функции main() вычислительного потока.

Функция MPI\_Init() возвращает значение константы компилятора MPI\_SUCCESS в случае своего успешного завершения и значение, отличное от значения указанной константы компилятора, в случае неудачного своего завершения. То же возвращают и все прочие функции стандарта MPI, рассмотренные в настоящей лабораторной работе, если только не указано обратное.

По завершении параллельной части кода вычислительный поток должен вызвать функцию MPI\_Finalize().

Вычислительный поток может получить свой номер, вызвав функцию MPI\_Comm\_rank(). Для получения числа потоков в определенной группе потоков вычислительный поток может воспользоваться функцией MPI\_Comm\_size() (листинг \ref{listing:mpi-comm-rank-size}).

\mylistingbegin{mpi-comm-rank-size}{Функции MPI\_Comm\_rank() и MPI\_Comm\_size()}
\begin{lstlisting}

int MPI_Comm_rank(MPI_COMM_WORLD, int * rank);

int MPI_Comm_size(MPI_COMM_WORLD, int * size);

\end{lstlisting}
\mylistingend

Функция MPI\_Comm\_rank() присвоит номер потока переменной, на которую указывает параметр rank. Функция MPI\_Comm\_\linebreak size() присвоит значение количества потоков в группе MPI\_COMM\_\linebreak WORLD переменной, на которую указывает параметр size.

Для отправки и приема сообщений вычислительные потоки могут использовать следующий функционал стандарта MPI:

\begin{itemize}

	\item функции MPI\_Send() и MPI\_Recv() (листинг \ref{listing:mpi-send-recv}).

\mylistingbegin{mpi-send-recv}{Функции MPI\_Send() и MPI\_Recv()}
\begin{lstlisting}

int MPI_Send(void * buf, int count, MPI_Datatype type, int dst, int tag, MPI_COMM_WORLD);

int MPI_Recv(void * buf, int count, MPI_Datatype type, int src, int tag, MPI_COMM_WORLD, MPI_Status * status);

\end{lstlisting}
\mylistingend

	Функция MPI\_Send() позволяет вычислительному потоку отправить сообщение целевому вычислительному потоку или всем вычислительным потокам, входящим в целевую группу потоков. Функция MPI\_Send() отправляет count элементов типа type из буфера buf. В качестве значения параметра type могут использоваться значения следующих констант компилятора:

	\begin{itemize}

		\item MPI\_CHAR - char;
		\item MPI\_SHORT - short int;
		\item MPI\_INT - int;
		\item MPI\_LONG - long int;
		\item MPI\_UNSIGNED\_CHAR - unsigned char;
		\item MPI\_UNSIGNED\_SHORT - unsigned short int;
		\item MPI\_UNSIGNED - unsigned int;
		\item MPI\_UNSIGNED\_LONG - unsigned long int;
		\item MPI\_FLOAT - float;
		\item MPI\_DOUBLE - double;
		\item MPI\_LONG\_DOUBLE - long double.

	\end{itemize}

	Таким образом, функция MPI\_Send() отправит целевому вычислительному потоку (потокам) первые (count * sizeof(ctype)) байт из буфера buf потока - отправителя, где ctype - тип языка программирования C, соответствующий значению type.

	Значение параметра dst функции MPI\_Send() суть есть номер того вычислительного потока, которому отправляется сообщение. Для широковещательной рассылки сообщений в качестве значения параметра dst необходимо указать значение константы компилятора MPI\_ANY\_SOURCE.

	Значение параметра tag указывает тег сообщения. Тег сообщение - положительное целое число, идентифицирующее данное сообщение (тип данного сообщения).

	Поток - супервизор помещает сообщение, отправленное некоторому вычислительному потоку, в очередь сообщений данного потока, из которого оно может быть выбрано потоком с помощью функции MPI\_Recv(). При этом соблюдается временная очередность выборки сообщений из очереди - первым будет выбрано то сообщение от указанного источника с указанным тегом, которое было помещено в очередь раньше остальных.

	Функция MPI\_Recv() принимает сообщение с тегом tag от вычислительного потока с номером src, помещая count элементов типа type сообщения в буфер buf. В переменную, на которую указывает параметр status, помещается информация о параметрах принятого сообщения. В случае, если сообщение с указанным тегом от указанного источника в очереди сообщений отсутствует, вычислительный поток блокируется в ожидании такового сообщения. Для приема сообщений с любым тегом в качестве значения параметра tag необходимо указать значение константы компилятора MPI\_ANY\_TAG. Для приема сообщений от любого источника в качестве значения параметра src необходимо указать значение константы компилятора MPI\_ANY\_SOURCE;

	\item функции MPI\_Bcast() и MPI\_Gather() (листинг \ref{listing:mpi-bcast-gather}).

\mylistingbegin{mpi-bcast-gather}{Функции MPI\_Bcast() и MPI\_Gather()}
\begin{lstlisting}

int MPI_Bcast(void * buf, int count, MPI_Datatype type, int src, MPI_COMM_WORLD);

int MPI_Gather(void * src_buf, int src_count, MPI_Datatype src_type, void * dst_buf, int dst_count, MPI_Datatype dst_type, int dst, MPI_COMM_WORLD);

\end{lstlisting}
\mylistingend

	Функция MPI\_Bcast() позволяет организовать широковещательную рассылку сообщения. Означенная функция отправляет count элементов типа type из буфера buf все вычислительным потокам из группы потоков MPI\_COMM\_WORLD. Номер потока, отправляющего сообщение, указывается параметром src. Данный поток также получит отправляемое им сообщение.
	
	Функция MPI\_Gather() позволяет организовать сбор данных от всех вычислительных потоков одним из них. Вычислительный поток с номером dst принимает сообщения от всех вычислительных потоков группы MPI\_COMM\_WORLD (в том числе, и от себя самого). Каждый вычислительный поток отправляет src\_count элементов типа src\_type из буфера src\_buf. Вычислительный поток с номером dst принимает по dst\_count элементов типа dst\_type от каждого вычислительного потока и помещает принятые данные в буфере dst\_buf, располагая блоки данных в порядке возрастания номеров отправивших их вычислительных потоков.

\end{itemize}

При разработке многопоточных приложений, использующих стандарт MPI, полезным будет также использование функции MPI\_\linebreak Barrier() (листинг \ref{listing:mpi-barrier}), позволяющей синхронизировать выполнение вычислительных потоков.

\mylistingbegin{mpi-barrier}{Функция MPI\_Barrier()}
\begin{lstlisting}

int MPI_Barrier(MPI_COMM_WORLD);

\end{lstlisting}
\mylistingend

Вычислительный поток, входящий в группу потоков MPI\_\linebreak COMM\_WORLD, блокируется функцией MPI\_Barrier() до тех пор, пока все остальные потоки данной группы не вызовут функцию MPI\_Barrier().

\mysubsubsection{Сборка и запуск программ}

В исходный код программы необходимо включить заголовочный файл mpi.h.

Для сборки программы с помощью компилятора GNU C Compiler необходимо использовать оболочку к означенному компилятору, пре\-доставляемую библиотекой OpenMPI. Данная оболочка присутствует в системе в виде исполняемого файла mpicc. Исполняемый файл mpicc запустит компилятор GNU C Compiler, добавить к аргументам, указанным программистом, аргументы, необходимые для корректного связывания программы с библиотекой OpenMPI.

Процесс запуска программы в вычислительном кластере состоит из следующих шагов:

\begin{enumerate}

	\item настройка среды запуска\footnote{В лаборатории настройка среды запуска выполняется лаборантами.}.

	Вычислительные потоки, запускаемые на отдельных вычислительных системах, входящих в состав кластера, обмениваются между собой сообщениями по тем или иным протоколам.

	Существуют протоколы обмена сообщениями, специально разработанные (и, следовательно, оптимизированные) для организации вычислительных кластеров. В случае же организации кластера путем объединения нескольких персональных компьютеров, соединенных друг с другом в вычислительную сеть, в качестве протоколов обмена сообщениями библиотекой OpenMPI используются протоколы RSH или SSH. Протокол SSH использует шифрование для защиты пересылаемых данных, что негативно сказывается на временных характеристиках процесса обмена сообщениями между вычислительными потоками.

	Следует помнить, что программа будет запущена на удаленных вычислительных системах от имени того же пользователя, от имени которого она запускается в головной вычислительной системе кластера. Таким образом, данный пользователь на удаленных вычислительных системах должен существовать и ему должен быть разрешен удаленный доступ без ввода пароля (в случае протокола SSH - аутентификация по ключу);

	\item сборка программы;

	\item рассылка исполняемого файла программы на удаленные вычислительные системы.

	Исполняемый файл программы может быть передан на удаленные вычислительные системы следующими способами:

	\begin{itemize}

		\item передачей на съемном носителе;
		\item передачей с помощью утилиты ncat (nc; при этом требуется удаленный доступ к целевой вычислительной системе по протоколам RSH или SSH);

	\end{itemize}

	\item запуск программы.

	Для запуска программы используется утилита mpirun, входящая в состав библиотеки OpenMPI. Утилита mpirun принимает следующие аргументы:

	\begin{itemize}

		\item -mca orte\_rsh\_agent rsh - использование для обмена сообщениями протокола RSH;
		\item -mca orte\_rsh\_agent ssh - использование для обмена сообщениями протокола SSH;
		\item -mca orte\_rsh\_agent \verb|"|ssh : rsh\verb|"| - использование для обмена сообщениями протокола SSH или, если подключение по SSH завершилось неудачей, протокола RSH;
		\item -n NUM - предписание запустить NUM вычислительных потоков;
		\item -H IP\_1,IP\_2,...,IP\_N - объединить в вычислительный кластер удаленные вычислительные системы с перечисленными IP-адресами;
		\item -x PATH=\verb|"|/bin:/usr/bin:/sbin:/usr/sbin:/sbin:DIR\verb|"| - предписание запускать вычислительные потоки с указанным значением переменной окружения PATH.

		Здесь DIR - абсолютный путь к каталогу, содержащему исполняемый файл программы. Если исполняемый файл программы в разных вычислительных системах находится в разных каталогах, то перечисление каталогов необходимо продолжить через двоеточие.

		В удаленных вычислительных системах исполняемый файл программы будет запущен с помощью утилиты orted, поэтому принципиально важно нахождение ее исполняемого файла в каталогах, перечисленных в переменной окружения PATH;

		\item имя исполняемого файла программы.

		Исполняемый файл программы должен находится в одном из каталогов, перечисленных в переменной PATH.

	\end{itemize}

	Стандартные потоки вывода и ошибок каждого вычислительного потока будут переправлены в тот терминал, в котором программа была запущена на выполнение с помощью утилиты mpirun.

\end{enumerate}

